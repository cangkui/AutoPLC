
# TODO：实现一个从编码和debug过程中学习经验策略的agent
import json
from autoplc_scl.agents.clients import OpenAIClient
from typing import List, Dict, Tuple, Any, Optional

class LearnAgent():
    """
    分别从相似案例、算法描述中推荐
    
    """
    # 定义一个类变量，用于存储基础日志文件夹的路径
    base_logs_folder: str = None

    @classmethod
    def extract_content(cls,response) -> str:
        """
        提取大模型响应的content
        """
        if hasattr(response, 'choices'):
            choice = response.choices[0]
            if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                content = choice.message.content
            else:
                content = choice['message']['content']
        else:
            content = response.content[0].text
        return content
    
    @classmethod
    def get_json_from_content(cls,content:str) : 
        try:
            json_start = content.index('```json') + len('```json')
            json_end = content.index('```', json_start)
            json_str = content[json_start:json_end].strip()
            json_data = json.loads(json_str)
        except (ValueError, json.JSONDecodeError) as e:
            print(f"Error parsing JSON from content: {e}")
            json_data = None
        return json_data

    @classmethod
    def run_learn_from_coding(cls, task: dict, prediction_scl: str, groundtruth_scl: str, openai_client:OpenAIClient) -> dict:
        """
        Summarize the differences and provide feedback based on the prediction and the groundtruth code.
        
        Args:
        task (dict): The task provided by the user.
        prediction_scl (str): The SCL code generated by AutoPLC.
        groundtruth_scl (str): The reference code provided by an expert.
        
        Returns:
        dict: A summary of strengths, weaknesses, and future improvement suggestions.
        """
        # Prepare the prompt based on the provided information
        learn_from_task_with_data = learn_from_task_user.format(
            task=str(task),
            prediction_scl=prediction_scl,
            groundtruth_scl=groundtruth_scl
        )
        
        # Call the LLM to analyze the task and provide a reflection
        response = openai_client.call(
            messages=[{"role": "system", "content": learn_from_task},{"role": "user", "content": learn_from_task_with_data}],
            task_name="learn_from_task",
            role_name="learn_agent"
        )
        
        # Extract the content from the LLM response
        content = cls.extract_content(response)
        
        # Parse the response content to get the structured feedback
        feedback_summary = cls.get_json_from_content(content)
        
        return feedback_summary


    @classmethod
    def run_learn_from_debug(cls, task: dict, groundtruth_scl: str, debug_history: List[dict], openai_client:OpenAIClient) -> dict:
        """
        Summarize the common errors based on the current task's the debug history.
        
        Args:
        task (dict): The task provided by the user.
        groundtruth_scl (str): The reference code provided by an expert.
        debug_history (List[dict]): List of debugging messages from the conversation history:
            {
            "scl_before_fix" : "", 
            "compiler" : "", // TIA Portal output
            "assistant" : "" // AI output
            }
        
        Returns:
        dict: A summary of errors and improvement suggestions.
        """
        # Prepare the prompt based on the provided information
        learn_from_complier_with_data = learn_from_complier_user.format(
            task=task,
            debug_history="\n".join([f"Debug Step: {step}" for step in debug_history]),
            groundtruth_scl=groundtruth_scl
        )
        
        # Call the LLM to analyze the task and provide a reflection
        response = openai_client.call(
            messages=[{"role": "system", "content": learn_from_complier},{"role": "user", "content": learn_from_complier_with_data}],
            task_name="learn_from_debug",
            role_name="learn_agent"
        )
        
        # Extract the content from the LLM response
        content = cls.extract_content(response)
        
        # Parse the response content to get the structured error details
        error_summary = cls.get_json_from_content(content)
        
        return error_summary


learn_from_complier = """
角色：你是一位经验丰富的 PLC 编程指导助手，擅长从编译错误与修复记录中总结经验，提炼出后续代码生成应注意的风险点与建议提示。

任务目标：
根据本次任务中遇到的编译错误及其最终的修复方式，总结出本类错误的通用触发条件、误用场景及下次应该提醒代码生成器（LogicComposer）避免的写法。

请以结构化形式输出以下内容：
```json
[{
    "错误类型": "简洁描述错误类别（如变量未声明、数据类型不匹配、函数参数缺失等）",
    "错误信息": "原始编译错误提示",
    "错误原因": "造成错误的本质原因",
    "修复方式": "本次修复采用的方式",
    "下次提示建议": "如何在生成代码时主动规避或提醒"
},{...}]
```
要求：
- 输出应针对“泛化”错误模式，而非仅限当前变量；
- 提示建议应适合用作 future system prompt 的内容，能直接注入到 LogicComposer；
- 多个错误可分条输出；
"""


learn_from_complier_user = """
## 需求
{task}

## Debug过程
{debug_history}

## 正确答案
{groundtruth_scl}

"""


learn_from_task = """
角色：你是一个能自我反思的 PLC 代码生成智能体，任务结束后需要复盘你的生成质量，并结合工程师提供的标准实现，总结你做得好的部分与需要改进的地方。

输入包括：
- 你所生成的代码片段
- 工程师实现的参考代码（ground truth）

任务目标：
请你分析两者在控制逻辑、变量使用、语义表达、结构合理性等方面的差异，并总结：
1. 哪些部分你完成得不错，说明你在哪些语义或逻辑层面理解到位；
2. 哪些部分存在问题，错在哪，可能的误解或遗漏是什么；
3. 下次你可以如何改进（提升表达清晰度、减少重复、增强可执行性等）。

输出格式建议：
```json
{
    "做得好的点": [...],
    "存在的问题": [...],
    "未来改进建议": [...]
}
```
要求：
- 对比应涵盖语义准确性、结构清晰度、变量合理性、冗余与缺漏；
- 输出建议应直接可用作 future system prompt 中的“反思补丁”；
- 如果模型对某块逻辑理解不清，应诚实说明；
"""


learn_from_task_user = """
## 需求
{task}

## 生成结果
{prediction_scl}

## 正确答案
{groundtruth_scl}

"""